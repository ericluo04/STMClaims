{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "611598e3",
   "metadata": {
    "id": "O9i4DPYfB5Xl"
   },
   "source": [
    "***\n",
    "*Homework 3: Knoll et al. (2015) + STM*\n",
    "***\n",
    "**Code Author:** Lan Luo  \n",
    "**Course:** Probabilistic Models and Machine Learning (Fall 2022)  \n",
    "**Professor:** David Blei\n",
    "<br>  \n",
    "This code processes data and cleans text related to thoughts that come to mind for retirement benefit claiming decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386912b6",
   "metadata": {
    "id": "OmlijscPB5Xn"
   },
   "source": [
    "# Initialize Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384246ab",
   "metadata": {
    "id": "viIKnIyIB5Xn"
   },
   "source": [
    "## Set File Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "903c9e96",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16356,
     "status": "ok",
     "timestamp": 1625161374702,
     "user": {
      "displayName": "Lan Luo",
      "photoUrl": "",
      "userId": "15582355578537095691"
     },
     "user_tz": 240
    },
    "id": "IAyHl1XxB5Xo",
    "outputId": "89900660-b634-40ac-c54e-27f893df7881"
   },
   "outputs": [],
   "source": [
    "# set paths\n",
    "base_path = 'G:/My Drive/Columbia Files'\n",
    "path = f'{base_path}/Coursework/2022-2023 Fall/Graphical Models/Homework 3'\n",
    "## for my helper functions\n",
    "helper_path = f'{base_path}/Research/- helper'\n",
    "\n",
    "# set today's date\n",
    "from datetime import date\n",
    "str_date = date.today()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75a77ab",
   "metadata": {
    "id": "C_b02s-XB5Xp"
   },
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da125dbc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 813,
     "status": "ok",
     "timestamp": 1625161386671,
     "user": {
      "displayName": "Lan Luo",
      "photoUrl": "",
      "userId": "15582355578537095691"
     },
     "user_tz": 240
    },
    "id": "Oc01TvU8B5Xp",
    "outputId": "4c24fe58-f4c4-4e81-889d-3aba95166f77"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'lan_cv' from 'G:/My Drive/Columbia Files/Research/- helper\\\\lan_cv.py'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# basic essentials\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "import ujson as json\n",
    "import random, sys, gzip, pickle, os, gc, itertools, time, importlib, re, scipy\n",
    "\n",
    "# data visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "sys.path.append(helper_path)\n",
    "from lan_vis import *\n",
    "importlib.reload(sys.modules['lan_vis'])\n",
    "%matplotlib inline\n",
    "vis_style('light')\n",
    "\n",
    "# text processing\n",
    "from lan_text import *\n",
    "importlib.reload(sys.modules['lan_text'])\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaMulticore, LdaModel\n",
    "from gensim.test.utils import datapath\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# computer vision\n",
    "from lan_cv import *\n",
    "importlib.reload(sys.modules['lan_cv'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6874a4b0",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74506628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "data = pd.read_csv(f\"{path}/1 SSA dataset; Study3; Nov 2021.csv\").dropna(subset=['study'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6ebc0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all text aspects\n",
    "aspects = [x for x in data if x.startswith('aspect_text')]\n",
    "# subset cols\n",
    "data = data[['S3_NATURAL_ORDER', 'CLAIM_AGE', 'PID', 'text_integrated',\n",
    "             'AGE', 'FEMALE', 'COLIVING', 'Kids', 'INCOME', 'EDUC', 'NUMERACY', 'SAVINGS', 'SMOKER', 'Whatisyourpoliticalaffiliation',\n",
    "             'RaceBlackorAfricanAmerican', 'RaceAmericanIndianorAlaskanNative', 'RaceWhite', \n",
    "             'RaceHispanic', 'RaceAsian', 'RaceHawaiianorPacificIslander', 'RaceOther',\n",
    "             'ELIGIBLE', 'SUBJ_HEALTH', 'SUBJ_LONG_RISK'] + aspects]\n",
    "\n",
    "# rename cols\n",
    "data.columns = [x.lower() for x in data.columns]\n",
    "data = data.rename(columns={\"s3_natural_order\": \"treatment\", \"pid\": \"ID\", \"text_integrated\": \"text_full\",\n",
    "                            \"educ\":\"education\", \"whatisyourpoliticalaffiliation\": \"politics\", \n",
    "                            \"raceblackorafricanamerican\": \"black\", \"raceamericanindianoralaskannative\": \"amerindian_alaskan\", \n",
    "                            \"racewhite\": \"white\", \"racehispanic\": \"hispanic\", \"raceasian\": \"asian\", \n",
    "                            \"racehawaiianorpacificislander\": \"hawaii_pacific\", \"raceother\": \"other\", \n",
    "                            \"eligible\": \"benefit_eligible\", \"subj_health\": \"perc_health\", \"subj_long_risk\": \"life_expectancy\"})\n",
    "\n",
    "# adjust missing values encoding\n",
    "data = data.replace('#NULL!', np.nan)\n",
    "\n",
    "# drop if no text\n",
    "data = data.dropna(subset=['text_full'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53f293fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# redefine treatment (so that 1 = unnatural order)\n",
    "data['treatment'] = np.abs(data['treatment'] - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fad3bc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine ethnicity columns\n",
    "data['ethnicity'] = np.nan\n",
    "ethnic_list = ['black', 'amerindian_alaskan', 'white', 'hispanic', 'asian', 'hawaii_pacific', 'other']\n",
    "for ethnic in ethnic_list:\n",
    "    data['ethnicity'] = np.where(data[ethnic]==\"Yes\", ethnic, data['ethnicity'])\n",
    "    \n",
    "data = data.drop(columns = ethnic_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c517cd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape wide to long (so that each thought is one row)\n",
    "data_long = pd.melt(data, id_vars=[x for x in data.columns if x not in aspects], value_vars=aspects)\n",
    "# drop missing thoughts\n",
    "data_long = data_long.dropna(subset=['value'])\n",
    "# sort by participant ID\n",
    "data_long = data_long.sort_values(by = [\"ID\", \"variable\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e82f3669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add thought count per participant\n",
    "data_long['thought_count'] = (data_long.groupby([\"ID\"])['variable'].transform('nunique'))\n",
    "data_long = data_long.rename(columns={\"value\": \"thought\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "621c55a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop melted col\n",
    "data_long = data_long.drop(columns=['variable'])\n",
    "# reorder cols\n",
    "data_long.insert(4, 'thought', data_long.pop(\"thought\"))\n",
    "data_long.insert(5, 'thought_count', data_long.pop(\"thought_count\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0b0569",
   "metadata": {},
   "source": [
    "# Clean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ebdbe24a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lower casing...\n",
      "Decoding html...\n",
      "Expanding contractions...\n",
      "Replacing punctuation with spaces...\n",
      "Removing stop words...\n",
      "\t['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'other', 'some', 'such', 'nor', 'only', 'own', 'same', 'so', 'than', 'too', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "\n",
      "Stemming...\n",
      "Filtering corpus-specific tokens...\n",
      "\tNum of unique tokens: 1,246\n",
      "\tAll common tokens: ['would', 'more']\n",
      "\tSample of rare tokens: ['father', 'equival', 'claim', 'elect', 'accumul', 'seek', 'reaso', 'qualiti', 'almost', '6', 'fall', 'own', 'movi', 'simpl', 'post', '22', 'moor', 'particip', 'befor', 'hold', 'stree', 'cours', 'mom', 'supliment', 'consider']\n",
      "\tNum of unique tokens after filtering: 440\n",
      "\n",
      "\n",
      "All done! ^-^\n"
     ]
    }
   ],
   "source": [
    "# clean description\n",
    "data_long = clean_text(data_long, 'thought', exclude_num=False, reduction=\"stem\",\n",
    "                       keep_useful_stop=True, exclude_stop=True,\n",
    "                       filter_freq=True, low_filter=2, common_filter=2)\n",
    "# reorder cols\n",
    "data_long.insert(6, 'thought_clean', data_long.pop(\"thought_clean\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "493a0481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as csv\n",
    "data_long.to_csv(\"claims_long.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d8f83f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
